The 'Eff' column is 'Eff'ective io rate in ms.  This is svctm (but "service time" isn't really service time as most storage folks think of it as).

It's calculated by %busy * iterval-ms) / (total-io-completed-in-interval).  While this is a reasonable compromise realize it still has all the
issues of 'svctm' in iostat.  Namely, io that continues into the next interval contributes to the current intervals busy time but not its completion
count. {Note the code does NOT track %busy but rather %idle -- and the inverse of that is %busy: e.g. disk is idle w/no io for 890ms out of 1000ms,
then it must have been busy 110ms or 11%.}

Note: this issue could be related to started but, due to lack of event matching, ended up being considered running throughout the interval.
Note: the match still doesn't seem to add up?  00:25 was busy 986.50ms out of 1000ms, a total of 14 io completed so why is the Eff. equivalent
      to busy * 1000ms / 1 ??? I'd have expected 986.50 / 14 value instead.  Need to figure out if this is feature or bug.

#SEC@MM:SS|  tot/s    r/s    w/s    d/s|   tkB/s   rkB/s   wkB/s   dkB/s|    avrq  qusz:max |    await:disk |  r_await:disk |  w_await:disk | d_await:disk  | Eff.   |  %busy | #reQ #Slp #Bar
#SEC@-----|-------+------+------+------|--------+-------+-------+-------|--------+-----:----|---------:-----|---------:-----|---------:-----|---------:-----|--------|--------|-----+----+----+
#SEC@00:00|      7      0      7      0|     128       0     128       0|   36.57   1.8   3 |     3.53 100% |     0.00   0% |     3.53 100% |     0.00   0% |  27.63 |   2.76 |    0    0    0
#SEC@00:02|      5      0      5      0|      24       0      24       0|    9.60   0.0   1 |     0.13  99% |     0.00   0% |     0.13  99% |     0.00   0% |  25.33 |   2.53 |    0    0    0
#SEC@00:05|      8      0      8      0|      86       0      86       0|   21.50   0.9   3 |     3.23 100% |     0.00   0% |     3.23 100% |     0.00   0% |  27.24 |   2.72 |    0    0    0
#SEC@00:09|      7      0      7      0|     104       0     104       0|   29.71   1.0   1 |     3.43 100% |     0.00   0% |     3.43 100% |     0.00   0% |  23.60 |   2.36 |    0    0    0
#SEC@00:10|     17      0     17      0|      96       0      96       0|   11.29   0.6   7 |     0.13  97% |     0.00   0% |     0.13  97% |     0.00   0% |   4.72 |   0.47 |    0    0    0
#SEC@00:15|     13      0     13      0|     100       0     100       0|   15.38   0.9   3 |     1.95 100% |     0.00   0% |     1.95 100% |     0.00   0% |  27.53 |   2.75 |    0    0    0
#SEC@00:19|      8      0      8      0|     184       0     184       0|   46.00   1.6   2 |     4.69 100% |     0.00   0% |     4.69 100% |     0.00   0% |  23.58 |   2.36 |    0    0    0
#SEC@00:20|     34      0     34      0|     172       0     172       0|   10.12   0.4   6 |     0.08  97% |     0.00   0% |     0.08  97% |     0.00   0% |   7.20 |   0.72 |    0    0    0
#SEC@00:22|      2      0      2      0|       8       0       8       0|    8.00   1.0   1 |    11.92 100% |     0.00   0% |    11.92 100% |     0.00   0% |  23.46 |   2.35 |    0    0    0
#SEC@00:23|      4      0      4      0|      40       0      40       0|   20.00   0.8   4 |     0.34  98% |     0.00   0% |     0.34  98% |     0.00   0% |   2.60 |   0.26 |    0    0    0
#SEC@00:25|     14      0     14      0|     212       0     212       0|   30.29   0.0   3 |     0.24  98% |     0.00   0% |     0.24  98% |     0.00   0% | 986.50 |  98.65 |    0    0    0


>> eff bad  

# --- File Summary nvme0n1.blktrace.txt ----------------------------------------------------------------------

#SUM: dTime(sec)         Q2I(ms)      Q2D(ms)      D2C(ms)      Q2C(ms;await) io_cnt avrq          KiB  reQ:       seeks prog flt
#SUM:     0 totals         0.000        0.451        0.000 %  0       171.748    119   19         1154    0:  6738824660    0   0
#SUM:     0 tot.avg        0.000        0.004        0.000 %  0         1.443    119   19            9    0:    56628778
#SUM:     0 reads          0.000        0.000        0.000 %  0         0.000      0    0            0    0
#SUM:     0 rd.avg         0.000        0.000        0.000 %  0         0.000      0    0            0    0
#SUM:     0 rd.max         0.000        0.000        0.000 %  0         0.000
#SUM:     0 writes         0.000        0.004        0.000 %  0       171.748    119   19         1154    0
#SUM:     0 wr.avg         0.000        0.004        0.000 %  0         1.443    119   19         1154    0
#SUM:     0 wr.max         0.000        0.019        0.000 %  0        23.661
#SUM:     0 discard        0.000        0.000        0.000 %  0         0.000      0    0            0    0
#SUM:     0 dx.avg         0.000        0.000        0.000 %  0         0.000      0    0            0    0
#SUM:     0 dx.max         0.000        0.000        0.000 %  0         0.000
#SUM:     0 idle        24820.612ms (idle:%2482.06, busy:%-2382.06)
#SUM@MM:SS|  tot/s    r/s    w/s    d/s|   tkB/s   rkB/s   wkB/s   dkB/s|    avrq  qusz:max |    await:disk |  r_await:disk |  w_await:disk | d_await:disk  | Eff.   |  %busy | #reQ #Slp #Bar
#SUM@-----|-------+------+------+------|--------+-------+-------+-------|--------+-----:----|---------:-----|---------:-----|---------:-----|---------:-----|--------|--------|-----+----+----+
#SUM@00:00|    119      0    119      0|    1154       0    1154       0|   19.39   0.0   7 |     1.44   0% |     0.00   0% |     1.44   0% |     0.00   0% |-23820.61 | -2382.06 |    0    0    0
#SUM:
#SUM:|--------|---------|-----------------------|----------------------|
#SUM:|   REMAP COUNTS   |  ______ Reads _______ |______ Writes _______ |
#SUM:|   From |   To    |    Count      Sectors |   Count      Sectors |
#SUM:|--------|---------|---------+-------------|---------+------------|
#SUM: 253,  3  259,  0           0            0       245         2296
#SUM: 259,  3  259,  0           0            0       225         2308
#SUM: 253,  0  259,  0           0            0         3           12
#
#

