#
#                   +-------------+
#                   |             |
#                   |     +---+   |
#                   |     |  {R} {R}
#                   |     |   ^   ^                                                       +----+
#                   v     v   |   |                                                       v    |
# A->Q+>{S}->G->{X}-+->I--+---+>D-+-------------------------------------------------------+->C-+--->
#     |            ^
#     v  +--->M----+
#     +--+--->F----+
#
# Key: { }- optional or unsual events that nominally should not be present
#       A - remAp   ; lba block mapping               M - Merge   ; merge io onto end of existing request on q
#       Q - Queue   ; io arrival @sched               F - Front   ; merge io onto front of existing request on q
#       S - Sleep   ; await for request               D - Dispatch; send req to driver
#       G - Getreq  ; get request struct              R - Requeue ; io rejected, place back on queue
#       I - Insert  ; insert req into q               C - Complete; io done
#       X - SplitIO ; split a request in two          B - Bounce  ; bounced data page to reserved lower memory req'd
#       -----------------------------------------------------------------------
#       P - Plug    ; plug/stop draining io to drv    U - Unplug  ; unplug drain
#       T - Timeout ; unplug drain due to timeout


The "normal" io event sequence of A,Q,G,I,D,C.  Sleeps may be added before a 'G' event to wait for request data structure
resources.  The 'D'ispatch can fail and be rejected.  In older kernel versions this resulted in a looping D,R,I... sequence
until the ending D,C events signifying a successful dispatch and complete. 

In newer kernels, the D,R,I... sequence is shortened to just R,R,R,...D,C.  The initial Dispatch is not present, nor an re-Insert,
instead just a Requeue implying a dispatch was attempted and a reinsert performed.  Note that the code will create ghost D and I
events in the case or R,R,R,R... sequences.  The code measures A|Q to the first D (or first ghost D generated by a Requeue) as 
being associated with the kernel processing time.  While the last D to C event is associated with storage time.  This leaves a 
D2D time (1st to last/final dispatch) time being not really kernel or storage but due to (likely) congestion issues within the 
driver or hba.


Completion events may be singular, as in the following:

#Maj,Mn CPU   SeqNo     Seconds     PID  Evt Typ Sector   +Len Description
#------ --- ------- --------------- -----|--|---|---------+--- -------------------------------------
259,0    3        1     0.000000000  1867  A   W 746579600 + 8 <- (253,3) 578805392   -> remap from dm-3 to nvme0n1p3 259,3 [FN.1]
259,0    3        2     0.000000149  1867  A   W 763360912 + 8 <- (259,3) 746579600   -> remap from nvme0n1p3 -> nvme0n1
259,0    3        3     0.000000299  1867  Q   W 763360912 + 8 [dmcrypt_write/2]      -> queue io 
259,0    3        4     0.000005301  1867  G   W 763360912 + 8 [dmcrypt_write/2]      -> get request
259,0    3        6     0.000006262  1867  A  WS 746579608 + 8 <- (253,3) 578805400   -> remap from dm-3 to nvme0n1p3
259,0    3        7     0.000006303  1867  A  WS 763360920 + 8 <- (259,3) 746579608   -> remap from nvme0n1p3 -> nvme0n1
259,0    3        8     0.000006341  1867  Q  WS 763360920 + 8 [dmcrypt_write/2]      -> queue
259,0    3        9     0.000006937  1867  M  WS 763360920 + 8 [dmcrypt_write/2]      -> merge this BIO to the end of the prior REQUEST
259,0    3       10     0.000009435  1867  D   W 763360912 + 16 [dmcrypt_write/2]     -> dispatch to driver/storage
259,0    3       11     0.023558761     0  C   W 763360912 + 16 [0]                   -> io done/completed

FN.1 Note in this case the remap indicates its from dm-3 nvme0n1 (259,0 in Maj,Mn columns).  But then the next merge is from nvmen1p3
     (259,3) to 259,0.  So the original remap 253,3 -> 259,0 is incorrect, its really 253,3 -> 259,3 and then a 2nd remap from partition
     2 to the base nvme0n1 (259,0) device.  This is a common bug in the event stream -- that is the remap points to the wrong device
     that is the target/destination of the remap.  Older kernels did not have this issue.
     brw-rw----. 1 root disk 259, 0 Nov 18 00:14 /dev/nvme0n1
     brw-rw----. 1 root disk 259, 1 Nov 18 00:14 /dev/nvme0n1p1
     brw-rw----. 1 root disk 259, 2 Nov 18 00:14 /dev/nvme0n1p2
     brw-rw----. 1 root disk 259, 3 Nov 18 00:14 /dev/nvme0n1p3

But completion events can be multiples also:

#Maj,Mn CPU   SeqNo     Seconds     PID  Evt Typ Sector   +Len Description
#------ --- ------- --------------- -----|--|---|---------+--- -------------------------------------
259,0   11        1     0.024124329  1889  A FWFS 575480360 + 0 <- (253,3) 407706152
259,0   11        2     0.024124474  1889  Q FWFS [jbd2/dm-4-8]
259,0   11        3     0.024126502  1889  G FWFS [jbd2/dm-4-8]
259,0   11        4     0.024133625   612  D  FN [kworker/11:1H]
259,0   11        5     0.024899010     0  C  FN 0 [0]				-> first complete
259,0   11        6     0.024903271     0  C WFS 575480360 [0]                  -> 2nd   complete (final completion event)



Other kernels generated a separate Completion event per BIO and a final one for the whole REQUEST.  In other cases the 
sequence was the opposite (whole Request first, then series of C per BIO events).  And in still others a series of C per
BIO and none for the whole request.  The matching code within the state machine accounts and processes each of these 
different event streams for an individual io context.


Note that Splits (X events) can removed io from the front or back of the io -- different kernel versions do it in
different ways.
